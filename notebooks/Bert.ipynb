{"cells":[{"cell_type":"code","source":"import codecs\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch import nn\nimport time\n\nimport random\nrandom.seed(0)\ntorch.manual_seed(0)\n\n# from transformers import BertTokenizer, BertModel\n# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n# bert_model = BertModel.from_pretrained(\"bert-base-cased\")\nimport matplotlib.pyplot as plt\n\nimport sys\nsys.path.insert(1, '/work/nlp-project')\nfrom scripts.read_write_data import read_processed_data, write_baseline_pred\nfrom scripts.evaluation_functions import f1_score, tag_accuracy\nfrom models.classes import DataIterator, Batch\n\nimport gensim.models\nGoogleEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n                                '/work/nlp-project/models/GoogleNews-50k.bin', binary=True)\n\nimport matplotlib.pyplot as plt","metadata":{"cell_id":"4e07c06972574f56940c8556d5a8d821","source_hash":"3103b69","execution_start":1683378668632,"execution_millis":8793,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# TRAIN_PATH = \"nlp-project/data/processed/train.conll\"\nTRAIN_PATH = \"/work/nlp-project/data/processed/train_splits/labeled.conll\"\nDEV_PATH = \"nlp-project/data/processed/dev.conll\"\nTEST_PATH = \"nlp-project/data/processed/test.conll\"\n\n# Loading data\n\nx_train = []\ny_train = []\nfor words, labels, _, _ in read_processed_data(TRAIN_PATH):\n    x_train.append(words)\n    y_train.append(labels)\n\nx_dev = []\ny_dev = []\nfor words, labels, _, _ in read_processed_data(DEV_PATH):\n    x_dev.append(words)\n    y_dev.append(labels)","metadata":{"cell_id":"fc1bd7847d6d48e2895b02bfb4b781ae","source_hash":"6946e600","execution_start":1683378681484,"execution_millis":296,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class BiLSTM(torch.nn.Module):\n    def __init__(self,\n                embedding_type = 'google',\n                LSTM_HIDDEN=20,\n                max_len=100,\n                n_labels=3,\n                batch_size=32,\n                pad_token=\"<PAD>\",\n                pad_label=2\n                ):\n        super().__init__()\n        \n        self.embedding_type = embedding_type  # 'bert' (doesn't work) or 'google' for where to get embeddings from\n        if embedding_type == 'google':\n            self.EMBEDDING_DIM = 300  # length of embedding vectors\n        elif embedding_type == 'bert':\n            self.EMBEDDING_DIM = 768  # length of embedding vectors\n        else:\n            raise AttributeError(\"Must\")\n        self.LSTM_HIDDEN = LSTM_HIDDEN  # number of LSTM cells\n        self.max_len=max_len  # maximum input sentence length, will be padded to this size\n        self.n_labels = n_labels\n        self.lstm = nn.LSTM(input_size=self.EMBEDDING_DIM, hidden_size=self.LSTM_HIDDEN, batch_first=True, bidirectional=True)\n        self.linear = nn.Linear(in_features=2 * self.LSTM_HIDDEN, out_features=n_labels)\n        self.batch_size = batch_size\n        self.pad_token = pad_token\n        self.pad_label = pad_label\n\n    def pad_inputs(self, collection: List[List[int]], padding_token):\n        to_series = [pd.Series(el) for el in collection]\n        enc_matrix = (pd.concat(to_series, axis=1)\n                        .reindex(range(self.max_len))\n                        .fillna(padding_token)\n                        .T)\n        collection = enc_matrix.values.tolist()\n        return collection\n\n    def forward(self, inputs):\n        '''\n        Implements a forward pass through the Bi-LSTM.\n        inputs are a batch (list) of sentences.\n        '''\n        if self.embedding_type == 'bert':\n            word_embeds = self._get_bert_embeds(inputs)\n        elif self.embedding_type == 'google':\n            word_embeds = self._get_google_embeds(inputs)\n\n        # word_embeds = nn.Dropout(p=0.2)(word_embeds)\n        lstm_result, _ = self.lstm(word_embeds)\n        # lstm_result = nn.Dropout(p=0.3)(lstm_result)\n        tags = self.linear(lstm_result)\n        log_probs = F.softmax(tags, dim=2)\n        return log_probs\n    \n    def _get_google_embeds(self, inputs):\n        embeddings = torch.Tensor()\n        for sentence in inputs:\n            sentence_embeds = torch.Tensor()\n            for word in sentence:\n                if GoogleEmbs.__contains__(word):\n                    embed = GoogleEmbs.get_vector(word)\n                    embed.setflags(write = True)\n                    embed = torch.from_numpy(embed)\n                else:\n                    embed = torch.zeros(300)  # the word is not in the model dictionary, so use zero vector\n                sentence_embeds = torch.cat((sentence_embeds, embed), dim=0)\n            embeddings = torch.cat((embeddings, sentence_embeds), dim=0)\n        return embeddings.view(len(inputs), -1, self.EMBEDDING_DIM)\n\n    # def _get_bert_embeds(self, inputs):\n    #     embeddings = torch.Tensor().float()\n    #     for sentence in inputs:\n\n    #         input_ids = torch.Tensor([bert_tokenizer.convert_tokens_to_ids(sentence)]).long()\n    #         sentence_embeds = bert_model(input_ids)[0][0].float()\n\n    #         embeddings = torch.cat((embeddings, sentence_embeds), dim=0).float()\n    #     return embeddings.view(len(inputs), -1, self.EMBEDDING_DIM).float()\n    \n    def fit(self, documents, labels, dev, LEARNING_RATE=0.01, EPOCHS=3):\n        \n    \n        self.train()\n\n        # Padding data\n        padded_documents = self.pad_inputs(documents, self.pad_token)\n        padded_labels = self.pad_inputs(labels, self.pad_label)\n        padded_labels = [list(map(int,sentence)) for sentence in padded_labels]\n        if dev:\n            dev_docs, dev_labs = dev\n            padded_dev_docs = self.pad_inputs(dev_docs, self.pad_token)\n            padded_dev_labs = self.pad_inputs(dev_labs, self.pad_label)\n            padded_dev_labs = [list(map(int,s)) for s in padded_dev_labs]\n\n        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n        loss_func = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=self.pad_label)  # ignores loss for padding token\n        data_iterator = DataIterator(batch_size=self.batch_size)\n    \t\n        # Logs for performance\n        train_f1_log = []\n        dev_f1_log = [],\n\n        for epoch in range(EPOCHS):\n            \n            total_tags = 0\n            matched_tags = 0\n            epoch_loss = 0\n            batch_f1s = []\n\n            for i, batch in enumerate(data_iterator(padded_documents, padded_labels)):\n\n                pred_tags = self.forward(inputs=batch.inputs)\n                pred_tags = pred_tags.view(-1, self.n_labels) # probability distribution for each tag across all words in batch\n                targets = torch.tensor(batch.targets).flatten()  # true label for each word\n                batch_loss = loss_func(pred_tags, targets)\n                epoch_loss += batch_loss.item()\n                \n                # backprop\n                batch_loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                batch_f1 = f1_score(targets, pred_tags, pad_label=self.pad_label, relaxed=True)  # exact match f1\n                batch_f1s.append(batch_f1)\n\n                # batch_acc = tag_accuracy(targets, pred_tags, pad_label=self.pad_label)\n\n            train_epoch_f1 = round(np.mean(batch_f1s), 3)  # mean f1 over batches in epoch\n            train_f1_log.append(train_epoch_f1)\n\n            print(f\"Epoch {epoch}, train F1:{train_f1_log[epoch]}, loss: {epoch_loss:.3f}\")\n            # print(f\"Epoch {epoch}, F1: {train_epoch_f1:.3f}, loss: {epoch_loss:.3f}\")\n","metadata":{"cell_id":"95e83014787a460cb09bf0cf465f1bbd","source_hash":"c584a86d","execution_start":1683378684021,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Training model:","metadata":{"cell_id":"c19f28d122804b85a8a472a3f6fe68b2","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"model = BiLSTM(LSTM_HIDDEN=5)\nmodel.fit(x_train, y_train, dev=None, LEARNING_RATE=0.1, EPOCHS=10)","metadata":{"cell_id":"0ab1b285aeb248b9883a7dfc53540caa","source_hash":"378512a7","execution_start":1683378691256,"execution_millis":243773,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 0, train F1:1.0, loss: 63776.308\nEpoch 1, train F1:1.0, loss: 63508.484\nEpoch 2, train F1:1.0, loss: 63508.484\nEpoch 3, train F1:1.0, loss: 63508.483\nEpoch 4, train F1:1.0, loss: 63508.483\nEpoch 5, train F1:1.0, loss: 63508.483\nEpoch 6, train F1:1.0, loss: 63508.483\nEpoch 7, train F1:1.0, loss: 63508.483\nEpoch 8, train F1:1.0, loss: 63508.483\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m BiLSTM(LSTM_HIDDEN\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [4], line 112\u001b[0m, in \u001b[0;36mBiLSTM.fit\u001b[0;34m(self, documents, labels, dev, LEARNING_RATE, EPOCHS)\u001b[0m\n\u001b[1;32m    108\u001b[0m batch_f1s \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_iterator(padded_documents, padded_labels)):\n\u001b[0;32m--> 112\u001b[0m     pred_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     pred_tags \u001b[38;5;241m=\u001b[39m pred_tags\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_labels) \u001b[38;5;66;03m# probability distribution for each tag across all words in batch\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch\u001b[38;5;241m.\u001b[39mtargets)\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# true label for each word\u001b[39;00m\n","Cell \u001b[0;32mIn [4], line 46\u001b[0m, in \u001b[0;36mBiLSTM.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     word_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bert_embeds(inputs)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     word_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_google_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# word_embeds = nn.Dropout(p=0.2)(word_embeds)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m lstm_result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(word_embeds)\n","Cell \u001b[0;32mIn [4], line 66\u001b[0m, in \u001b[0;36mBiLSTM._get_google_embeds\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m             embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m300\u001b[39m)  \u001b[38;5;66;03m# the word is not in the model dictionary, so use zero vector\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m         sentence_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embeddings, sentence_embeds), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(inputs), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEMBEDDING_DIM)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":5},{"cell_type":"code","source":"\"\"\"\nFull training data:\nlr=.1, LSTM_HIDDEN=1, 16 epochs, train F1:    0.634\nlr=.1, LSTM_HIDDEN=5, 16 epochs, train F1:    0.641\nlr=0.07, LSTM_HIDDEN=10, 16 epochs, train F1: 0.70\nlr=.05, LSTM_HIDDEN=10, 16 epochs, train F1:  0.718 (0.73 at 38 epochs)\n\n\"\"\"","metadata":{"cell_id":"bad1580ca4a84869a5fe227def296acb","source_hash":"5e92c01f","execution_start":1682277267317,"execution_millis":1101350818,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"code","source":"train_f1 = [0.0, 0.0, 0.288, 0.593, 0.609, 0.613, 0.617, 0.618, 0.619, 0.626, 0.626, 0.632, 0.637, 0.638, 0.632, 0.641, 0.64, 0.637, 0.636, 0.64]\n\nplt.plot(train_f1)","metadata":{"cell_id":"dbfcbf357b40445e83a876a93c72ef28","source_hash":"c4ca1162","execution_start":1682277267326,"execution_millis":1101350816,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"code","source":"","metadata":{"cell_id":"b038a398386f41269b4d48c0127cf490","source_hash":"b623e53d","execution_start":1682277267602,"execution_millis":1101350541,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"code","source":"","metadata":{"cell_id":"ffa4c500ef6d412b84f54ff9b19e5edb","source_hash":"b623e53d","execution_start":1682277267607,"execution_millis":1101350536,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b2f14aee-af04-4db5-af55-57a3a58b9f40' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"e6f1f21c8d454f699f939151ea3d0ac5","deepnote_execution_queue":[]}}