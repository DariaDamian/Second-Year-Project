{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "8c9048df2e7a4147a7d2506bd4633f87",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 27162,
    "execution_start": 1685010079428,
    "source_hash": "4b13f2f1"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Sequence, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "import gensim.models\n",
    "GoogleEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                                'nlp_project/models/GoogleNews-50k.bin', binary=True)\n",
    "import random\n",
    "\n",
    "# Ensuring reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"nlp_project\")\n",
    "from nlp_project.scripts.read_write_data import read_processed_data, read_raw_data, load_data\n",
    "from nlp_project.models.classes import Batch, DataIterator, PolyDataIterator, F1_evaluator, F1_error_evaluator, WeightedCrossEntropy #Train1BiLSTM #BaselineBiLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "a2c8eab292a645bb8727d4b9cffb1290",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 106,
    "execution_start": 1685010106607,
    "source_hash": "9ec58ccf"
   },
   "outputs": [],
   "source": [
    "class Train1BiLSTM(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                hidden_size=20,\n",
    "                max_len=100,\n",
    "                n_labels=3,\n",
    "                batch_size=32,\n",
    "                pad_token=\"<PAD>\",\n",
    "                pad_label=2,\n",
    "                embedding_dim=300\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim  # length of embedding vectors\n",
    "        self.hidden_size = hidden_size  # number of LSTM cells\n",
    "        self.max_len=max_len  # maximum input sentence length, will be padded to this size\n",
    "        self.n_labels = n_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_label = pad_label\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(in_features=2 * self.hidden_size, out_features=n_labels)\n",
    "\n",
    "        self.data_iterator = DataIterator(batch_size=self.batch_size)\n",
    "\n",
    "        # Logs for performance in each training epoch\n",
    "        self.train_f1_log = []\n",
    "        self.dev_f1_log = []\n",
    "\n",
    "    def _pad_inputs(self, collection: List[List[int]], padding_token):\n",
    "        to_series = [pd.Series(el) for el in collection]\n",
    "        enc_matrix = (pd.concat(to_series, axis=1)\n",
    "                        .reindex(range(self.max_len))\n",
    "                        .fillna(padding_token)\n",
    "                        .T)\n",
    "        collection = enc_matrix.values.tolist()\n",
    "        return collection\n",
    "\n",
    "    def _pad_data(self, documents, labels):\n",
    "        padded_documents = self._pad_inputs(documents, self.pad_token)\n",
    "        padded_labels = self._pad_inputs(labels, self.pad_label)\n",
    "        padded_labels = [list(map(int,sentence)) for sentence in padded_labels]\n",
    "        return padded_documents, padded_labels\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Implements a forward pass through the BiLSTM.\n",
    "        inputs are a batch (list) of sentences.\n",
    "        '''\n",
    "        word_embeds = self._get_google_embeds(inputs)\n",
    "        lstm_result, _ = self.lstm(word_embeds)\n",
    "        tags = self.linear(lstm_result)\n",
    "        log_probs = torch.nn.functional.softmax(tags, dim=2)\n",
    "        return log_probs\n",
    "    \n",
    "    def _get_google_embeds(self, inputs):\n",
    "        embeddings = torch.Tensor()\n",
    "        for sentence in inputs:\n",
    "            sentence_embeds = torch.Tensor()\n",
    "            for word in sentence:\n",
    "                if GoogleEmbs.__contains__(word):\n",
    "                    embed = GoogleEmbs.get_vector(word)\n",
    "                    embed.setflags(write = True)\n",
    "                    embed = torch.from_numpy(embed)\n",
    "                else:\n",
    "                    embed = torch.zeros(self.embedding_dim)  # the word is not in the model dictionary, so use zero vector\n",
    "                sentence_embeds = torch.cat((sentence_embeds, embed), dim=0)\n",
    "            embeddings = torch.cat((embeddings, sentence_embeds), dim=0)\n",
    "        return embeddings.view(len(inputs), -1, self.embedding_dim)\n",
    "    \n",
    "    def fit(self, train, dev=None, epochs=3, print_metrics=False, learning_rate=0.05):\n",
    "        \n",
    "        documents, labels = train\n",
    "        padded_documents, padded_labels = self._pad_data(documents, labels)  # Padding training data\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=self.pad_label)  # ignores loss for padding label\n",
    "        Evaluator = F1_evaluator(self.pad_label)\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            self.train()\n",
    "            for i, batch in enumerate(self.data_iterator(padded_documents, padded_labels)):\n",
    "                pred_tags = self.forward(inputs=batch.inputs)\n",
    "                # pred_tags = pred_tags.view(-1, self.n_labels) # probability distribution for each tag across all words in batch\n",
    "                targets = torch.tensor(batch.targets)  # \n",
    "                # print(f\"train batch shapes: {targets.shape, pred_tags.shape}\")\n",
    "                Evaluator.pass_batch(targets, pred_tags)  # passing batch labels to evaluator\n",
    "                batch_loss = loss_func(pred_tags.permute(0,2,1), targets)\n",
    "                epoch_loss += batch_loss.item()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            train_metrics = Evaluator.metrics\n",
    "            train_f1 = Evaluator.f1_score()\n",
    "            self.train_f1_log.append(train_f1)\n",
    "\n",
    "            if dev is None:  # print performance and go to next epoch if no dev data is supplied\n",
    "                if print_metrics:\n",
    "                    COR, PAR, INC, MIS, SPU, ACT, POS = list(train_metrics)\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f} \\n train metrics: {ACT} ACT, {POS} POS, {COR} COR, {PAR} PAR, {INC} INC ({MIS} MIS, {SPU} SPU)\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, loss: {epoch_loss:.3f}\")\n",
    "            else:\n",
    "                # Dev evaluation\n",
    "                x_dev, y_dev = dev\n",
    "                dev_f1, dev_metrics = self._dev_evaluate(x_dev, y_dev)\n",
    "                self.dev_f1_log.append(dev_f1)\n",
    "                if print_metrics:\n",
    "                    COR, PAR, INC, MIS, SPU, ACT, POS = list(dev_metrics)\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f} \\n dev metrics: {ACT} ACT, {POS} POS, {COR} COR, {INC} INC ({PAR} PAR, {MIS} MIS, {SPU} SPU)\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f}\")\n",
    "\n",
    "    def _dev_evaluate(self, x_dev, y_dev):\n",
    "        \"\"\"\n",
    "        Evaluates model performance on supplied data.\n",
    "        print_metrics set to print out metrics by default.\n",
    "        return_metrics (optionally) returns F1 and metrics.\n",
    "        \"\"\"\n",
    "        padded_dev_docs, padded_dev_labs = self._pad_data(x_dev, y_dev)  # Padding data\n",
    "        self.eval()\n",
    "        Evaluator = F1_evaluator(self.pad_label)\n",
    "        for i, batch in enumerate(self.data_iterator(padded_dev_docs, padded_dev_labs)):\n",
    "            with torch.no_grad():\n",
    "                pred_dev = self.forward(batch.inputs)\n",
    "            targets = torch.tensor(batch.targets)\n",
    "            # print(f\"dev batch shapes: {targets.shape, pred_dev.shape}\")\n",
    "            Evaluator.pass_batch(targets, pred_dev)\n",
    "        dev_metrics = Evaluator.metrics\n",
    "        dev_f1 = Evaluator.f1_score()\n",
    "        return dev_f1, dev_metrics\n",
    "\n",
    "    def evaluate(self, x_dev, y_dev, BIOlabels, domains, print_metrics=True, return_errors=True):\n",
    "        \"\"\"\n",
    "        Evaluates model performance on supplied data.\n",
    "        print_metrics set to print out f1, precision, recall and metrics by default.\n",
    "        return_errors returns error DataFrame by default.\n",
    "        \"\"\"\n",
    "        padded_dev_docs, padded_dev_labs = self._pad_data(x_dev, y_dev)  # Padding data\n",
    "        self.eval()\n",
    "        Evaluator = F1_error_evaluator(x_dev, BIOlabels, domains, pad_label=self.pad_label)\n",
    "        for i, batch in enumerate(self.data_iterator(padded_dev_docs, padded_dev_labs)):\n",
    "            with torch.no_grad():\n",
    "                pred_dev = self.forward(batch.inputs)\n",
    "            targets = torch.tensor(batch.targets)\n",
    "            Evaluator.pass_batch(targets, pred_dev)\n",
    "        dev_metrics = Evaluator.metrics\n",
    "        error_df = Evaluator.errors\n",
    "        precision, recall, f1 = Evaluator.f1_score(verbose=True)\n",
    "        if print_metrics:\n",
    "            print(f\"F1: {f1:.3f} precision: {precision:.3f} recall: {recall:.3f}\")\n",
    "            COR, PAR, INC, MIS, SPU, ACT, POS = list(dev_metrics)\n",
    "            print(f\"Metrics: {ACT} ACT, {POS} POS, {COR} COR, {INC} INC ({PAR} PAR, {MIS} MIS, {SPU} SPU)\")\n",
    "        if return_errors:\n",
    "            # error_df[\"general_label\"] = error_df[\"BIO_label\"][0]\n",
    "            return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "cb6bd3c2d99b4850afd7071b9ad7e558",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 39,
    "execution_start": 1685010106720,
    "source_hash": "83562a67"
   },
   "outputs": [],
   "source": [
    "class Train2BiLSTM(Train1BiLSTM):\n",
    "    def __init__(self,\n",
    "                hidden_size=20,\n",
    "                max_len=100,\n",
    "                n_labels=3,\n",
    "                batch_size=32,\n",
    "                pad_token=\"<PAD>\",\n",
    "                pad_label=2,\n",
    "                embedding_dim=300\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim  # length of embedding vectors\n",
    "        self.hidden_size = hidden_size  # number of LSTM cells\n",
    "        self.max_len=max_len  # maximum input sentence length, will be padded to this size\n",
    "        self.n_labels = n_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_label = pad_label\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(in_features=2 * self.hidden_size, out_features=n_labels)\n",
    "\n",
    "        self.poly_data_iterator = PolyDataIterator(batch_size=self.batch_size)\n",
    "        self.data_iterator = DataIterator(batch_size=self.batch_size)\n",
    "\n",
    "        # Logs for performance in each training epoch\n",
    "        self.train_f1_log = []\n",
    "        self.dev_f1_log = []\n",
    "    \n",
    "    def fit(self, \n",
    "            train,\n",
    "            train2, \n",
    "            dev=None,\n",
    "            epochs=20, \n",
    "            print_metrics=False, \n",
    "            learning_rate=0.005,\n",
    "            alpha=None):\n",
    "        \n",
    "        documents, labels = train\n",
    "        pseudo_docs, pseudo_labels = train2\n",
    "\n",
    "        # padding training data\n",
    "        padded_documents, padded_labels = self._pad_data(documents, labels) \n",
    "        pseudo_docs, pseudo_labels = self._pad_data(pseudo_docs, pseudo_labels)\n",
    "\n",
    "        loss_func = WeightedCrossEntropy(epochs, pad_label=self.pad_label)\n",
    "        Evaluator = F1_evaluator(self.pad_label)\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            self.train()\n",
    "            for labeled, paraphrased in self.poly_data_iterator([padded_documents, pseudo_docs], \n",
    "                                                                [padded_labels, pseudo_labels]):\n",
    "                pred_tags = self.forward(inputs=labeled[0])\n",
    "                pred_pseudo_tags = self.forward(inputs=paraphrased[0])\n",
    "                # probability distribution for each tag across all words\n",
    "                # pred_tags = pred_tags.view(-1, self.n_labels)\n",
    "                # pred_pseudo_tags = pred_pseudo_tags.view(-1, self.n_labels)\n",
    "                # true label for each word\n",
    "                targets = torch.tensor(labeled[1])  # .flatten()\n",
    "                pseudo_targets = torch.tensor(paraphrased[1])  # .flatten()\n",
    "                # passing all batch labels to evaluator\n",
    "                Evaluator.pass_batch(torch.cat((targets, pseudo_targets), 0), \n",
    "                                     torch.cat((pred_tags, pred_pseudo_tags), 0))\n",
    "                batch_loss = loss_func(pred_tags.permute(0,2,1), pred_pseudo_tags.permute(0,2,1), targets, pseudo_targets, alpha=(alpha or epoch / epochs))\n",
    "                epoch_loss += batch_loss.item()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            train_metrics = Evaluator.metrics\n",
    "            train_p, train_r, train_f1 = Evaluator.f1_score(verbose=True)\n",
    "            self.train_f1_log.append(train_f1)\n",
    "\n",
    "            if dev is None:  # print performance and go to next epoch if no dev data is supplied\n",
    "                if print_metrics:\n",
    "                    COR, PAR, INC, MIS, SPU, ACT, POS = list(train_metrics)\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f} \\n train metrics: {ACT} ACT, {POS} POS, {COR} COR, {PAR} PAR, {INC} INC ({MIS} MIS, {SPU} SPU)\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, loss: {epoch_loss:.3f}\")\n",
    "            else:\n",
    "                # Dev evaluation\n",
    "                x_dev, y_dev = dev\n",
    "                dev_f1, dev_metrics = self._dev_evaluate(x_dev, y_dev)\n",
    "                self.dev_f1_log.append(dev_f1)\n",
    "                if print_metrics:\n",
    "                    COR, PAR, INC, MIS, SPU, ACT, POS = list(dev_metrics)\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f} \\n dev metrics: {ACT} ACT, {POS} POS, {COR} COR, {PAR} PAR, {INC} INC ({MIS} MIS, {SPU} SPU)\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "60641b38e8714869842767cafdad5d33",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1719,
    "execution_start": 1685010106769,
    "source_hash": "408c7384"
   },
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"nlp_project/data/processed/train_splits/labeled.conll\"\n",
    "DEV_SET_PATH = \"nlp_project/data/processed/dev.conll\"\n",
    "PARAPHRASED_PATH = \"nlp_project/data/paraphrased/train_labeled.conll\"\n",
    "\n",
    "train_docs, train_labels, train_bio, train_domain = load_data(TRAIN_SET_PATH)\n",
    "dev_docs, dev_labels, dev_bio, dev_domain = load_data(DEV_SET_PATH)\n",
    "\n",
    "pp_docs = []\n",
    "pp_labels = []\n",
    "for words, labels in read_raw_data(PARAPHRASED_PATH):\n",
    "    pp_docs.append(words)\n",
    "    pp_labels.append(labels)\n",
    "pp_bio = train_bio\n",
    "pp_domain = train_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "909c21192c0e4bd391d872eef130fe2c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1685010108517,
    "source_hash": "595279ed"
   },
   "outputs": [],
   "source": [
    "# model = Train1BiLSTM(hidden_size=10)\n",
    "# model.fit(train = (train_docs, train_labels),\n",
    "#           dev = (dev_docs, dev_labels),\n",
    "#           print_metrics=True, \n",
    "#           learning_rate=0.005,\n",
    "#           epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "5e4c3abcc5594ffe8e73d0c2ae0c0289",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1685010108560,
    "source_hash": "a21266be"
   },
   "outputs": [],
   "source": [
    "# error_df = model.evaluate(dev_docs, dev_labels, dev_bio, dev_domain)\n",
    "# error_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "bcb8e3889c974a9d98cea7f152a0b656",
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 0,
     "pageSize": 50,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 239804,
    "execution_start": 1685010108560,
    "source_hash": "5e9b7c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train: 0.001, dev: 0.000 \n",
      " dev metrics: 0 ACT, 1368 POS, 0 COR, 0 PAR, 1368 INC (1368 MIS, 0 SPU)\n"
     ]
    }
   ],
   "source": [
    "model2 = Train2BiLSTM(hidden_size=10)\n",
    "model2.fit(train = (train_docs, train_labels),\n",
    "          train2 = (pp_docs, pp_labels),\n",
    "          dev = (dev_docs, dev_labels),\n",
    "          print_metrics=True, \n",
    "          learning_rate=0.005,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "89c0ed635f79475cb993a9ba68fc0c10",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16513,
    "execution_start": 1685010348363,
    "source_hash": "937b9a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.000 precision: 0.000 recall: 0.000\n",
      "Metrics: 0 ACT, 1368 POS, 0 COR, 1368 INC (0 PAR, 1368 MIS, 0 SPU)\n"
     ]
    },
    {
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 6,
       "columns": [
        {
         "dtype": "object",
         "name": "error_type",
         "stats": {
          "categories": [
           {
            "count": 5,
            "name": "MIS"
           }
          ],
          "nan_count": 0,
          "unique_count": 1
         }
        },
        {
         "dtype": "object",
         "name": "entity",
         "stats": {
          "categories": [
           {
            "count": 1,
            "name": "Washington Post"
           },
           {
            "count": 1,
            "name": "Anthony Shadid"
           },
           {
            "count": 3,
            "name": "3 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "object",
         "name": "entity_BIO",
         "stats": {
          "categories": [
           {
            "count": 2,
            "name": "['B-ORG']"
           },
           {
            "count": 1,
            "name": "['B-ORG', 'I-ORG']"
           },
           {
            "count": 2,
            "name": "2 others"
           }
          ],
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "object",
         "name": "sentence",
         "stats": {
          "categories": [
           {
            "count": 2,
            "name": "['Anthony', 'Shadid', 'of', 'the', 'Washington', 'Post', 'reveals', 'that', 'the', 'warrants', 'for', 'the', 'arrests', 'had', 'been', 'issued', 'months', 'be', 'for', '.']"
           },
           {
            "count": 2,
            "name": "['I', \"'m\", 'not', 'even', 'in', 'Iraq', 'and', 'I', 'could', 'have', 'predicted', 'to', 'you', 'the', 'consequences', 'of', 'doing', 'what', 'the', 'CPA', 'has', 'been', 'doing', '.']"
           },
           {
            "count": 1,
            "name": "['The', 'Army', 'is', 'unlikely', 'to', 'forgive', 'or', 'forget', ';', 'but', 'who', 'provoked', 'it', 'and', 'why', '?']"
           }
          ],
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "object",
         "name": "sentence_BIO",
         "stats": {
          "categories": [
           {
            "count": 2,
            "name": "['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
           },
           {
            "count": 2,
            "name": "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']"
           },
           {
            "count": 1,
            "name": "['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
           }
          ],
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "object",
         "name": "domain",
         "stats": {
          "categories": [
           {
            "count": 5,
            "name": "weblogs"
           }
          ],
          "nan_count": 0,
          "unique_count": 1
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "row_count": 5,
       "rows": [
        {
         "_deepnote_index_column": "0",
         "domain": "weblogs",
         "entity": "Washington Post",
         "entity_BIO": "['B-ORG', 'I-ORG']",
         "error_type": "MIS",
         "sentence": "['Anthony', 'Shadid', 'of', 'the', 'Washington', 'Post', 'reveals', 'that', 'the', 'warrants', 'for', 'the', 'arrests', 'had', 'been', 'issued', 'months', 'be', 'for', '.']",
         "sentence_BIO": "['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
        },
        {
         "_deepnote_index_column": "1",
         "domain": "weblogs",
         "entity": "Anthony Shadid",
         "entity_BIO": "['B-PER', 'I-PER']",
         "error_type": "MIS",
         "sentence": "['Anthony', 'Shadid', 'of', 'the', 'Washington', 'Post', 'reveals', 'that', 'the', 'warrants', 'for', 'the', 'arrests', 'had', 'been', 'issued', 'months', 'be', 'for', '.']",
         "sentence_BIO": "['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
        },
        {
         "_deepnote_index_column": "2",
         "domain": "weblogs",
         "entity": "CPA",
         "entity_BIO": "['B-ORG']",
         "error_type": "MIS",
         "sentence": "['I', \"'m\", 'not', 'even', 'in', 'Iraq', 'and', 'I', 'could', 'have', 'predicted', 'to', 'you', 'the', 'consequences', 'of', 'doing', 'what', 'the', 'CPA', 'has', 'been', 'doing', '.']",
         "sentence_BIO": "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']"
        },
        {
         "_deepnote_index_column": "3",
         "domain": "weblogs",
         "entity": "Iraq",
         "entity_BIO": "['B-LOC']",
         "error_type": "MIS",
         "sentence": "['I', \"'m\", 'not', 'even', 'in', 'Iraq', 'and', 'I', 'could', 'have', 'predicted', 'to', 'you', 'the', 'consequences', 'of', 'doing', 'what', 'the', 'CPA', 'has', 'been', 'doing', '.']",
         "sentence_BIO": "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']"
        },
        {
         "_deepnote_index_column": "4",
         "domain": "weblogs",
         "entity": "Army",
         "entity_BIO": "['B-ORG']",
         "error_type": "MIS",
         "sentence": "['The', 'Army', 'is', 'unlikely', 'to', 'forgive', 'or', 'forget', ';', 'but', 'who', 'provoked', 'it', 'and', 'why', '?']",
         "sentence_BIO": "['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
        }
       ]
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_type</th>\n",
       "      <th>entity</th>\n",
       "      <th>entity_BIO</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_BIO</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIS</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>[B-ORG, I-ORG]</td>\n",
       "      <td>[Anthony, Shadid, of, the, Washington, Post, r...</td>\n",
       "      <td>[B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...</td>\n",
       "      <td>weblogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIS</td>\n",
       "      <td>Anthony Shadid</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>[Anthony, Shadid, of, the, Washington, Post, r...</td>\n",
       "      <td>[B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...</td>\n",
       "      <td>weblogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MIS</td>\n",
       "      <td>CPA</td>\n",
       "      <td>[B-ORG]</td>\n",
       "      <td>[I, 'm, not, even, in, Iraq, and, I, could, ha...</td>\n",
       "      <td>[O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>weblogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIS</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>[B-LOC]</td>\n",
       "      <td>[I, 'm, not, even, in, Iraq, and, I, could, ha...</td>\n",
       "      <td>[O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>weblogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MIS</td>\n",
       "      <td>Army</td>\n",
       "      <td>[B-ORG]</td>\n",
       "      <td>[The, Army, is, unlikely, to, forgive, or, for...</td>\n",
       "      <td>[O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>weblogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  error_type           entity      entity_BIO  \\\n",
       "0        MIS  Washington Post  [B-ORG, I-ORG]   \n",
       "1        MIS   Anthony Shadid  [B-PER, I-PER]   \n",
       "2        MIS              CPA         [B-ORG]   \n",
       "3        MIS             Iraq         [B-LOC]   \n",
       "4        MIS             Army         [B-ORG]   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [Anthony, Shadid, of, the, Washington, Post, r...   \n",
       "1  [Anthony, Shadid, of, the, Washington, Post, r...   \n",
       "2  [I, 'm, not, even, in, Iraq, and, I, could, ha...   \n",
       "3  [I, 'm, not, even, in, Iraq, and, I, could, ha...   \n",
       "4  [The, Army, is, unlikely, to, forgive, or, for...   \n",
       "\n",
       "                                        sentence_BIO   domain  \n",
       "0  [B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...  weblogs  \n",
       "1  [B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...  weblogs  \n",
       "2  [O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...  weblogs  \n",
       "3  [O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...  weblogs  \n",
       "4  [O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...  weblogs  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df = model2.evaluate(dev_docs, dev_labels, dev_bio, dev_domain)\n",
    "error_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b2f14aee-af04-4db5-af55-57a3a58b9f40' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6172df7939e84ca7a2a3de561e0c44ea",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
