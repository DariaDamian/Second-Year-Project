{"cells":[{"cell_type":"code","source":"import codecs\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Sequence, Callable\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\nimport gensim.models\nGoogleEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n                                '/work/nlp-project/models/GoogleNews-50k.bin', binary=True)\nimport random\n\n# Ensuring reproducibility\nseed = 0\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(seed)\nrandom.seed(seed)\n\nimport sys\nsys.path.insert(1, '/work/nlp-project')\nfrom scripts.read_write_data import read_processed_data, read_raw_data, load_data\nfrom models.classes import Batch, DataIterator, PolyDataIterator, F1_evaluator, F1_error_evaluator, WeightedCrossEntropy #Train1BiLSTM #BaselineBiLSTM\n","metadata":{"cell_id":"8c9048df2e7a4147a7d2506bd4633f87","source_hash":"4b13f2f1","execution_start":1685010079428,"execution_millis":27162,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class Train1BiLSTM(torch.nn.Module):\n    def __init__(self,\n                hidden_size=20,\n                max_len=100,\n                n_labels=3,\n                batch_size=32,\n                pad_token=\"<PAD>\",\n                pad_label=2,\n                embedding_dim=300\n                ):\n        super().__init__()\n        \n        self.embedding_dim = embedding_dim  # length of embedding vectors\n        self.hidden_size = hidden_size  # number of LSTM cells\n        self.max_len=max_len  # maximum input sentence length, will be padded to this size\n        self.n_labels = n_labels\n        self.batch_size = batch_size\n        self.pad_token = pad_token\n        self.pad_label = pad_label\n\n        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, batch_first=True, bidirectional=True)\n        self.linear = nn.Linear(in_features=2 * self.hidden_size, out_features=n_labels)\n\n        self.data_iterator = DataIterator(batch_size=self.batch_size)\n\n        # Logs for performance in each training epoch\n        self.train_f1_log = []\n        self.dev_f1_log = []\n\n    def _pad_inputs(self, collection: List[List[int]], padding_token):\n        to_series = [pd.Series(el) for el in collection]\n        enc_matrix = (pd.concat(to_series, axis=1)\n                        .reindex(range(self.max_len))\n                        .fillna(padding_token)\n                        .T)\n        collection = enc_matrix.values.tolist()\n        return collection\n\n    def _pad_data(self, documents, labels):\n        padded_documents = self._pad_inputs(documents, self.pad_token)\n        padded_labels = self._pad_inputs(labels, self.pad_label)\n        padded_labels = [list(map(int,sentence)) for sentence in padded_labels]\n        return padded_documents, padded_labels\n\n    def forward(self, inputs):\n        '''\n        Implements a forward pass through the BiLSTM.\n        inputs are a batch (list) of sentences.\n        '''\n        word_embeds = self._get_google_embeds(inputs)\n        lstm_result, _ = self.lstm(word_embeds)\n        tags = self.linear(lstm_result)\n        log_probs = torch.nn.functional.softmax(tags, dim=2)\n        return log_probs\n    \n    def _get_google_embeds(self, inputs):\n        embeddings = torch.Tensor()\n        for sentence in inputs:\n            sentence_embeds = torch.Tensor()\n            for word in sentence:\n                if GoogleEmbs.__contains__(word):\n                    embed = GoogleEmbs.get_vector(word)\n                    embed.setflags(write = True)\n                    embed = torch.from_numpy(embed)\n                else:\n                    embed = torch.zeros(self.embedding_dim)  # the word is not in the model dictionary, so use zero vector\n                sentence_embeds = torch.cat((sentence_embeds, embed), dim=0)\n            embeddings = torch.cat((embeddings, sentence_embeds), dim=0)\n        return embeddings.view(len(inputs), -1, self.embedding_dim)\n    \n    def fit(self, train, dev=None, epochs=3, print_metrics=False, learning_rate=0.05):\n        \n        documents, labels = train\n        padded_documents, padded_labels = self._pad_data(documents, labels)  # Padding training data\n\n        loss_func = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=self.pad_label)  # ignores loss for padding label\n        Evaluator = F1_evaluator(self.pad_label)\n        optimizer = torch.optim.Adam(params=self.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            epoch_loss = 0\n            self.train()\n            for i, batch in enumerate(self.data_iterator(padded_documents, padded_labels)):\n                pred_tags = self.forward(inputs=batch.inputs)\n                # pred_tags = pred_tags.view(-1, self.n_labels) # probability distribution for each tag across all words in batch\n                targets = torch.tensor(batch.targets)  # \n                # print(f\"train batch shapes: {targets.shape, pred_tags.shape}\")\n                Evaluator.pass_batch(targets, pred_tags)  # passing batch labels to evaluator\n                batch_loss = loss_func(pred_tags.permute(0,2,1), targets)\n                epoch_loss += batch_loss.item()\n                batch_loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n            train_metrics = Evaluator.metrics\n            train_f1 = Evaluator.f1_score()\n            self.train_f1_log.append(train_f1)\n\n            if dev is None:  # print performance and go to next epoch if no dev data is supplied\n                if print_metrics:\n                    COR, PAR, INC, MIS, SPU, ACT, POS = list(train_metrics)\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f} \\n train metrics: {ACT} ACT, {POS} POS, {COR} COR, {PAR} PAR, {INC} INC ({MIS} MIS, {SPU} SPU)\")\n                else:\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, loss: {epoch_loss:.3f}\")\n            else:\n                # Dev evaluation\n                x_dev, y_dev = dev\n                dev_f1, dev_metrics = self._dev_evaluate(x_dev, y_dev)\n                self.dev_f1_log.append(dev_f1)\n                if print_metrics:\n                    COR, PAR, INC, MIS, SPU, ACT, POS = list(dev_metrics)\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f} \\n dev metrics: {ACT} ACT, {POS} POS, {COR} COR, {INC} INC ({PAR} PAR, {MIS} MIS, {SPU} SPU)\")\n                else:\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f}\")\n\n    def _dev_evaluate(self, x_dev, y_dev):\n        \"\"\"\n        Evaluates model performance on supplied data.\n        print_metrics set to print out metrics by default.\n        return_metrics (optionally) returns F1 and metrics.\n        \"\"\"\n        padded_dev_docs, padded_dev_labs = self._pad_data(x_dev, y_dev)  # Padding data\n        self.eval()\n        Evaluator = F1_evaluator(self.pad_label)\n        for i, batch in enumerate(self.data_iterator(padded_dev_docs, padded_dev_labs)):\n            with torch.no_grad():\n                pred_dev = self.forward(batch.inputs)\n            targets = torch.tensor(batch.targets)\n            # print(f\"dev batch shapes: {targets.shape, pred_dev.shape}\")\n            Evaluator.pass_batch(targets, pred_dev)\n        dev_metrics = Evaluator.metrics\n        dev_f1 = Evaluator.f1_score()\n        return dev_f1, dev_metrics\n\n    def evaluate(self, x_dev, y_dev, BIOlabels, domains, print_metrics=True, return_errors=True):\n        \"\"\"\n        Evaluates model performance on supplied data.\n        print_metrics set to print out f1, precision, recall and metrics by default.\n        return_errors returns error DataFrame by default.\n        \"\"\"\n        padded_dev_docs, padded_dev_labs = self._pad_data(x_dev, y_dev)  # Padding data\n        self.eval()\n        Evaluator = F1_error_evaluator(x_dev, BIOlabels, domains, pad_label=self.pad_label)\n        for i, batch in enumerate(self.data_iterator(padded_dev_docs, padded_dev_labs)):\n            with torch.no_grad():\n                pred_dev = self.forward(batch.inputs)\n            targets = torch.tensor(batch.targets)\n            Evaluator.pass_batch(targets, pred_dev)\n        dev_metrics = Evaluator.metrics\n        error_df = Evaluator.errors\n        precision, recall, f1 = Evaluator.f1_score(verbose=True)\n        if print_metrics:\n            print(f\"F1: {f1:.3f} precision: {precision:.3f} recall: {recall:.3f}\")\n            COR, PAR, INC, MIS, SPU, ACT, POS = list(dev_metrics)\n            print(f\"Metrics: {ACT} ACT, {POS} POS, {COR} COR, {INC} INC ({PAR} PAR, {MIS} MIS, {SPU} SPU)\")\n        if return_errors:\n            # error_df[\"general_label\"] = error_df[\"BIO_label\"][0]\n            return error_df","metadata":{"cell_id":"a2c8eab292a645bb8727d4b9cffb1290","source_hash":"9ec58ccf","execution_start":1685010106607,"execution_millis":106,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Train2BiLSTM(Train1BiLSTM):\n    def __init__(self,\n                hidden_size=20,\n                max_len=100,\n                n_labels=3,\n                batch_size=32,\n                pad_token=\"<PAD>\",\n                pad_label=2,\n                embedding_dim=300\n                ):\n        super().__init__()\n        self.embedding_dim = embedding_dim  # length of embedding vectors\n        self.hidden_size = hidden_size  # number of LSTM cells\n        self.max_len=max_len  # maximum input sentence length, will be padded to this size\n        self.n_labels = n_labels\n        self.batch_size = batch_size\n        self.pad_token = pad_token\n        self.pad_label = pad_label\n\n        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, batch_first=True, bidirectional=True)\n        self.linear = nn.Linear(in_features=2 * self.hidden_size, out_features=n_labels)\n\n        self.poly_data_iterator = PolyDataIterator(batch_size=self.batch_size)\n        self.data_iterator = DataIterator(batch_size=self.batch_size)\n\n        # Logs for performance in each training epoch\n        self.train_f1_log = []\n        self.dev_f1_log = []\n    \n    def fit(self, \n            train,\n            train2, \n            dev=None,\n            epochs=20, \n            print_metrics=False, \n            learning_rate=0.005,\n            alpha=None):\n        \n        documents, labels = train\n        pseudo_docs, pseudo_labels = train2\n\n        # padding training data\n        padded_documents, padded_labels = self._pad_data(documents, labels) \n        pseudo_docs, pseudo_labels = self._pad_data(pseudo_docs, pseudo_labels)\n\n        loss_func = WeightedCrossEntropy(epochs, pad_label=self.pad_label)\n        Evaluator = F1_evaluator(self.pad_label)\n        optimizer = torch.optim.Adam(params=self.parameters(), lr=learning_rate)\n        \n        for epoch in range(epochs):\n            epoch_loss = 0\n            self.train()\n            for labeled, paraphrased in self.poly_data_iterator([padded_documents, pseudo_docs], \n                                                                [padded_labels, pseudo_labels]):\n                pred_tags = self.forward(inputs=labeled[0])\n                pred_pseudo_tags = self.forward(inputs=paraphrased[0])\n                # probability distribution for each tag across all words\n                # pred_tags = pred_tags.view(-1, self.n_labels)\n                # pred_pseudo_tags = pred_pseudo_tags.view(-1, self.n_labels)\n                # true label for each word\n                targets = torch.tensor(labeled[1])  # .flatten()\n                pseudo_targets = torch.tensor(paraphrased[1])  # .flatten()\n                # passing all batch labels to evaluator\n                Evaluator.pass_batch(torch.cat((targets, pseudo_targets), 0), \n                                     torch.cat((pred_tags, pred_pseudo_tags), 0))\n                batch_loss = loss_func(pred_tags.permute(0,2,1), pred_pseudo_tags.permute(0,2,1), targets, pseudo_targets, alpha=(alpha or epoch / epochs))\n                epoch_loss += batch_loss.item()\n                batch_loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n            train_metrics = Evaluator.metrics\n            train_p, train_r, train_f1 = Evaluator.f1_score(verbose=True)\n            self.train_f1_log.append(train_f1)\n\n            if dev is None:  # print performance and go to next epoch if no dev data is supplied\n                if print_metrics:\n                    COR, PAR, INC, MIS, SPU, ACT, POS = list(train_metrics)\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f} \\n train metrics: {ACT} ACT, {POS} POS, {COR} COR, {PAR} PAR, {INC} INC ({MIS} MIS, {SPU} SPU)\")\n                else:\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, loss: {epoch_loss:.3f}\")\n            else:\n                # Dev evaluation\n                x_dev, y_dev = dev\n                dev_f1, dev_metrics = self._dev_evaluate(x_dev, y_dev)\n                self.dev_f1_log.append(dev_f1)\n                if print_metrics:\n                    COR, PAR, INC, MIS, SPU, ACT, POS = list(dev_metrics)\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f} \\n dev metrics: {ACT} ACT, {POS} POS, {COR} COR, {PAR} PAR, {INC} INC ({MIS} MIS, {SPU} SPU)\")\n                else:\n                    print(f\"Epoch {epoch}, train: {train_f1:.3f}, dev: {dev_f1:.3f}\")","metadata":{"cell_id":"cb6bd3c2d99b4850afd7071b9ad7e558","source_hash":"83562a67","execution_start":1685010106720,"execution_millis":39,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"TRAIN_SET_PATH = \"nlp-project/data/processed/train_splits/labeled.conll\"\nDEV_SET_PATH = \"nlp-project/data/processed/dev.conll\"\nPARAPHRASED_PATH = \"nlp-project/data/paraphrased/train_labeled.conll\"\n\ntrain_docs, train_labels, train_bio, train_domain = load_data(TRAIN_SET_PATH)\ndev_docs, dev_labels, dev_bio, dev_domain = load_data(DEV_SET_PATH)\n\npp_docs = []\npp_labels = []\nfor words, labels in read_raw_data(PARAPHRASED_PATH):\n    pp_docs.append(words)\n    pp_labels.append(labels)\npp_bio = train_bio\npp_domain = train_domain","metadata":{"cell_id":"60641b38e8714869842767cafdad5d33","source_hash":"408c7384","execution_start":1685010106769,"execution_millis":1719,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# model = Train1BiLSTM(hidden_size=10)\n# model.fit(train = (train_docs, train_labels),\n#           dev = (dev_docs, dev_labels),\n#           print_metrics=True, \n#           learning_rate=0.005,\n#           epochs=1)","metadata":{"cell_id":"909c21192c0e4bd391d872eef130fe2c","source_hash":"595279ed","execution_start":1685010108517,"execution_millis":44,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# error_df = model.evaluate(dev_docs, dev_labels, dev_bio, dev_domain)\n# error_df.head(5)","metadata":{"cell_id":"5e4c3abcc5594ffe8e73d0c2ae0c0289","source_hash":"a21266be","execution_start":1685010108560,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model2 = Train2BiLSTM(hidden_size=10)\nmodel2.fit(train = (train_docs, train_labels),\n          train2 = (pp_docs, pp_labels),\n          dev = (dev_docs, dev_labels),\n          print_metrics=True, \n          learning_rate=0.005,\n          epochs=1)","metadata":{"cell_id":"bcb8e3889c974a9d98cea7f152a0b656","source_hash":"5e9b7c03","execution_start":1685010108560,"execution_millis":239804,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":50,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 0, train: 0.001, dev: 0.000 \n dev metrics: 0 ACT, 1368 POS, 0 COR, 0 PAR, 1368 INC (1368 MIS, 0 SPU)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"error_df = model2.evaluate(dev_docs, dev_labels, dev_bio, dev_domain)\nerror_df.head(5)","metadata":{"cell_id":"89c0ed635f79475cb993a9ba68fc0c10","source_hash":"937b9a66","execution_start":1685010348363,"execution_millis":16513,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"F1: 0.000 precision: 0.000 recall: 0.000\nMetrics: 0 ACT, 1368 POS, 0 COR, 1368 INC (0 PAR, 1368 MIS, 0 SPU)\n","output_type":"stream"},{"output_type":"execute_result","execution_count":8,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":6,"row_count":5,"columns":[{"name":"error_type","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"MIS","count":5}]}},{"name":"entity","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"Washington Post","count":1},{"name":"Anthony Shadid","count":1},{"name":"3 others","count":3}]}},{"name":"entity_BIO","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"['B-ORG']","count":2},{"name":"['B-ORG', 'I-ORG']","count":1},{"name":"2 others","count":2}]}},{"name":"sentence","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"['Anthony', 'Shadid', 'of', 'the', 'Washington', 'Post', 'reveals', 'that', 'the', 'warrants', 'for', 'the', 'arrests', 'had', 'been', 'issued', 'months', 'be', 'for', '.']","count":2},{"name":"['I', \"'m\", 'not', 'even', 'in', 'Iraq', 'and', 'I', 'could', 'have', 'predicted', 'to', 'you', 'the', 'consequences', 'of', 'doing', 'what', 'the', 'CPA', 'has', 'been', 'doing', '.']","count":2},{"name":"['The', 'Army', 'is', 'unlikely', 'to', 'forgive', 'or', 'forget', ';', 'but', 'who', 'provoked', 'it', 'and', 'why', '?']","count":1}]}},{"name":"sentence_BIO","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","count":2},{"name":"['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']","count":2},{"name":"['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","count":1}]}},{"name":"domain","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"weblogs","count":5}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"error_type":"MIS","entity":"Washington Post","entity_BIO":"['B-ORG', 'I-ORG']","sentence":"['Anthony', 'Shadid', 'of', 'the', 'Washington', 'Post', 'reveals', 'that', 'the', 'warrants', 'for', 'the', 'arrests', 'had', 'been', 'issued', 'months', 'be', 'for', '.']","sentence_BIO":"['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","domain":"weblogs","_deepnote_index_column":"0"},{"error_type":"MIS","entity":"Anthony Shadid","entity_BIO":"['B-PER', 'I-PER']","sentence":"['Anthony', 'Shadid', 'of', 'the', 'Washington', 'Post', 'reveals', 'that', 'the', 'warrants', 'for', 'the', 'arrests', 'had', 'been', 'issued', 'months', 'be', 'for', '.']","sentence_BIO":"['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","domain":"weblogs","_deepnote_index_column":"1"},{"error_type":"MIS","entity":"CPA","entity_BIO":"['B-ORG']","sentence":"['I', \"'m\", 'not', 'even', 'in', 'Iraq', 'and', 'I', 'could', 'have', 'predicted', 'to', 'you', 'the', 'consequences', 'of', 'doing', 'what', 'the', 'CPA', 'has', 'been', 'doing', '.']","sentence_BIO":"['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']","domain":"weblogs","_deepnote_index_column":"2"},{"error_type":"MIS","entity":"Iraq","entity_BIO":"['B-LOC']","sentence":"['I', \"'m\", 'not', 'even', 'in', 'Iraq', 'and', 'I', 'could', 'have', 'predicted', 'to', 'you', 'the', 'consequences', 'of', 'doing', 'what', 'the', 'CPA', 'has', 'been', 'doing', '.']","sentence_BIO":"['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']","domain":"weblogs","_deepnote_index_column":"3"},{"error_type":"MIS","entity":"Army","entity_BIO":"['B-ORG']","sentence":"['The', 'Army', 'is', 'unlikely', 'to', 'forgive', 'or', 'forget', ';', 'but', 'who', 'provoked', 'it', 'and', 'why', '?']","sentence_BIO":"['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","domain":"weblogs","_deepnote_index_column":"4"}]},"text/plain":"  error_type           entity      entity_BIO  \\\n0        MIS  Washington Post  [B-ORG, I-ORG]   \n1        MIS   Anthony Shadid  [B-PER, I-PER]   \n2        MIS              CPA         [B-ORG]   \n3        MIS             Iraq         [B-LOC]   \n4        MIS             Army         [B-ORG]   \n\n                                            sentence  \\\n0  [Anthony, Shadid, of, the, Washington, Post, r...   \n1  [Anthony, Shadid, of, the, Washington, Post, r...   \n2  [I, 'm, not, even, in, Iraq, and, I, could, ha...   \n3  [I, 'm, not, even, in, Iraq, and, I, could, ha...   \n4  [The, Army, is, unlikely, to, forgive, or, for...   \n\n                                        sentence_BIO   domain  \n0  [B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...  weblogs  \n1  [B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...  weblogs  \n2  [O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...  weblogs  \n3  [O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...  weblogs  \n4  [O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...  weblogs  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>error_type</th>\n      <th>entity</th>\n      <th>entity_BIO</th>\n      <th>sentence</th>\n      <th>sentence_BIO</th>\n      <th>domain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MIS</td>\n      <td>Washington Post</td>\n      <td>[B-ORG, I-ORG]</td>\n      <td>[Anthony, Shadid, of, the, Washington, Post, r...</td>\n      <td>[B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...</td>\n      <td>weblogs</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MIS</td>\n      <td>Anthony Shadid</td>\n      <td>[B-PER, I-PER]</td>\n      <td>[Anthony, Shadid, of, the, Washington, Post, r...</td>\n      <td>[B-PER, I-PER, O, O, B-ORG, I-ORG, O, O, O, O,...</td>\n      <td>weblogs</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MIS</td>\n      <td>CPA</td>\n      <td>[B-ORG]</td>\n      <td>[I, 'm, not, even, in, Iraq, and, I, could, ha...</td>\n      <td>[O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...</td>\n      <td>weblogs</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MIS</td>\n      <td>Iraq</td>\n      <td>[B-LOC]</td>\n      <td>[I, 'm, not, even, in, Iraq, and, I, could, ha...</td>\n      <td>[O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O,...</td>\n      <td>weblogs</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MIS</td>\n      <td>Army</td>\n      <td>[B-ORG]</td>\n      <td>[The, Army, is, unlikely, to, forgive, or, for...</td>\n      <td>[O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n      <td>weblogs</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b2f14aee-af04-4db5-af55-57a3a58b9f40' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"6172df7939e84ca7a2a3de561e0c44ea","deepnote_execution_queue":[]}}