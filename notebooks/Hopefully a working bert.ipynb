{"cells":[{"cell_type":"code","source":"import sys\nsys.path.insert(1, '/work/nlp-project')\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom transformers import BertTokenizer, BertModel\n\nfrom scripts.read_write_data import read_processed_data","metadata":{"cell_id":"b6de60a4e17e4f5fb52439267dfdf695","source_hash":"a9b8c517","execution_start":1681926322114,"execution_millis":2043,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}],"execution_count":1},{"cell_type":"code","source":"TRAIN_PATH = \"nlp-project/data/processed/train.conll\"\nDEV_PATH = \"nlp-project/data/processed/dev.conll\"\nTEST_PATH = \"nlp-project/data/processed/test.conll\"\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"cell_id":"de90f8e95e3e41e4af45c600fb26dbb7","source_hash":"b57094a8","execution_start":1681926324109,"execution_millis":48,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=False)\nbert_model = BertModel.from_pretrained(\"bert-base-cased\")","metadata":{"cell_id":"4dbd6b8fe5bf4107a067506221fd1fb8","source_hash":"ddf42a33","execution_start":1681926324109,"execution_millis":1792,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"sentences = []\ntargets = []\nfor words, labels, _, _ in read_processed_data(TRAIN_PATH):\n    sentences.append(words)\n    targets.append(list(map(int, labels)))","metadata":{"cell_id":"5a84ef53a3204e858b15f101d774788b","source_hash":"f05a1996","execution_start":1681926325917,"execution_millis":758,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"print(sentences[0])\nprint(targets[0])","metadata":{"cell_id":"5f082096d8b2432ab22b0f264c70d5a7","source_hash":"22b4a458","execution_start":1681926326630,"execution_millis":52,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"['My', 'dad', 'just', 'does', \"n't\", 'understand', '?']\n[0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from typing import List\n\ndef add_bert_tags(documents: List[List[str]], doc_tags: List[List[str]]):\n    updated_docs = [['[CLS]', *doc, '[SEP]'] for doc in documents]\n    updated_tags = [[0, *tags, \n] for tags in doc_tags]  # normally this would be <pad>, *tags, <pad>\n\n    return updated_docs, updated_tags\n\nbert_sentences, bert_targets = add_bert_tags(sentences, targets)\n\nprint(bert_sentences[0])\nprint(bert_targets[0])","metadata":{"cell_id":"21de81906e2446e9bfa790d9bcb6a5ea","source_hash":"d3de494d","execution_start":1681926326655,"execution_millis":28,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"['[CLS]', 'My', 'dad', 'just', 'does', \"n't\", 'understand', '?', '[SEP]']\n[0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Data Loader","metadata":{"cell_id":"ba4cdc6cdb2242b680d0df070c29339a","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from torch.utils import data\n\nclass BertDataset(data.Dataset):\n\n    def __init__(self, documents: List[List[str]], doc_tags: List[List[str]]):\n        self.docs = documents\n        self.doc_tags = doc_tags\n\n    def __len__(self):\n        return len(self.docs)\n\n    def __getitem__(self, idx: int):\n        sentence, tags = self.docs[idx], self.doc_tags[idx]\n\n        enc_sentence = []\n        enc_tags = []\n        is_heads = []\n        for word, tag in zip(sentence, tags):\n            tokens = bert_tokenizer.tokenize(word)\n            ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n\n            tag = [tag] + [0] * (len(tokens) - 1)  # target label should only be assigned to head\n            is_head = [1] + [0] * (len(tokens) - 1)  # pay attention only to the first part of the word\n\n            enc_sentence += ids\n            enc_tags += tag\n            is_heads += is_head\n        \n        sentence = \" \".join(sentence)\n        return sentence, enc_sentence, enc_tags\n\ndataset_test = BertDataset([bert_sentences[0]], [bert_targets[0]])\ndataset_test[0]","metadata":{"cell_id":"464903c512614bbf80472eb679ce9499","source_hash":"2faf5d91","execution_start":1681926326666,"execution_millis":16,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(\"[CLS] My dad just does n't understand ? [SEP]\",\n [101, 1422, 4153, 1198, 1674, 183, 112, 189, 2437, 136],\n [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def pad_batch(batch):\n    batch_t = list(map(list, zip(*batch)))  # transpose\n    sentences, enc_sents, enc_targets = batch_t\n    max_len = max((map(len, enc_sents)))\n    \n    pad_sents = []\n    pad_targets = []\n    for enc_sent, enc_target in zip(enc_sents, enc_targets):\n        pad_sents.append(enc_sent + [0] * (max_len - len(enc_sent)))\n        pad_targets.append(enc_target + [0] * (max_len - len(enc_target)))\n    \n    pad_sents = torch.LongTensor(pad_sents)\n    pad_targets = torch.LongTensor(pad_targets)\n\n    return sentences, pad_sents, pad_targets","metadata":{"cell_id":"f40d655964194e8eaa1acfade34dd3a4","source_hash":"1e724b8b","execution_start":1681926326685,"execution_millis":4,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Model","metadata":{"cell_id":"b4641fda37db4179a18a886d42a64cf0","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"# N - number of samples, T - number of tokens, EMB - embedding dimension\nbert_model(torch.LongTensor([dataset_test[0][1]]))[0].shape  # (N, T, EMB)","metadata":{"cell_id":"a0b05fc1c2ad47fe876426282afe48c1","source_hash":"15311f01","execution_start":1681926326722,"execution_millis":305,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"torch.Size([1, 10, 768])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import torch.nn.functional as F\n\ntorch.manual_seed(1)\n\n# TODO: come up with an original name for the model lol\nclass SomeModel(torch.nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(768, out_features=2)\n        \n    def forward(self, inputs):\n        \"\"\"\n        inputs: (N, T)\n        \"\"\"\n        embeds = bert_model(inputs)[0]   # (N, T, 768)\n        preds = self.linear(embeds)  # (N, T, 2)\n        log_probs = F.softmax(preds, dim=2)\n        return log_probs","metadata":{"cell_id":"e40ec59468d84b6bb66730298539eb49","source_hash":"3a61c6ad","execution_start":1681926327040,"execution_millis":5,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):          \n    model.train()\n\n    for i, batch in enumerate(iterator):\n        sent, enc_sent, enc_targets = batch\n\n        # predictions\n        pred_tags = model(inputs=enc_sent) # (N, T, 2)\n        pred_tags = pred_tags.view(-1, 2) # (N x T, 2)\n        \n        # true label for each word\n        targets = enc_targets.flatten()  # (N x T)\n\n        # loss\n        batch_loss = criterion(pred_tags, targets)\n        \n        # optimization\n        batch_loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n                        \n        if i % 10 == 0:\n            print(f\"Batch {i} loss: {batch_loss:.4f}\")","metadata":{"cell_id":"14cdf21e16014fb09853a71009d115cd","source_hash":"24ce0219","execution_start":1681926327112,"execution_millis":6,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Training","metadata":{"cell_id":"1706b67328544422a20403d63455ae92","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.optim import Adam\n\n\nmodel = SomeModel()\nmodel.to(device)\n\ntrain_dataset = BertDataset(\n    documents=bert_sentences,\n    doc_tags=bert_targets    \n)\n\ntrain_iter = data.DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n    shuffle=True,\n    collate_fn=pad_batch\n)\n\noptimizer = Adam(model.parameters(), lr=0.001)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)","metadata":{"cell_id":"854c66bcd8004e298d177e90b629b160","source_hash":"bb596e33","execution_start":1681926327122,"execution_millis":74,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train(model, train_iter, optimizer, criterion)","metadata":{"cell_id":"4766f79816184a0e9e4e445914956733","source_hash":"76577255","execution_start":1681926327178,"execution_millis":8607,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'F' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [11], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m sent, enc_sent, enc_targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# predictions\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pred_tags \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_sent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (N, T, 2)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pred_tags \u001b[38;5;241m=\u001b[39m pred_tags\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_labels) \u001b[38;5;66;03m# (N x T, 2)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# true label for each word\u001b[39;00m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn [10], line 16\u001b[0m, in \u001b[0;36mSomeModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     14\u001b[0m embeds \u001b[38;5;241m=\u001b[39m bert_model(inputs)[\u001b[38;5;241m0\u001b[39m]   \u001b[38;5;66;03m# (N, T, 768)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(embeds)  \u001b[38;5;66;03m# (N, T, 2)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39msoftmax(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_probs\n","\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"]}],"execution_count":13},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b2f14aee-af04-4db5-af55-57a3a58b9f40' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"46d7453ef7a841518deeb53b9fe33eb4","deepnote_execution_queue":[]}}