{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "b6de60a4e17e4f5fb52439267dfdf695",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 2043,
    "execution_start": 1681926322114,
    "source_hash": "a9b8c517"
   },
   "outputs": [
    {
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"nlp_project\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from scripts.read_write_data import read_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "de90f8e95e3e41e4af45c600fb26dbb7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 48,
    "execution_start": 1681926324109,
    "source_hash": "b57094a8"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"nlp_project/data/processed/train.conll\"\n",
    "DEV_PATH = \"nlp_project/data/processed/dev.conll\"\n",
    "TEST_PATH = \"nlp_project/data/processed/test.conll\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "4dbd6b8fe5bf4107a067506221fd1fb8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1792,
    "execution_start": 1681926324109,
    "source_hash": "ddf42a33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=False)\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "5a84ef53a3204e858b15f101d774788b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 758,
    "execution_start": 1681926325917,
    "source_hash": "f05a1996"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "targets = []\n",
    "for words, labels, _, _ in read_processed_data(TRAIN_PATH):\n",
    "    sentences.append(words)\n",
    "    targets.append(list(map(int, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "5f082096d8b2432ab22b0f264c70d5a7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 52,
    "execution_start": 1681926326630,
    "source_hash": "22b4a458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'dad', 'just', 'does', \"n't\", 'understand', '?']\n",
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "21de81906e2446e9bfa790d9bcb6a5ea",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 28,
    "execution_start": 1681926326655,
    "source_hash": "d3de494d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'dad', 'just', 'does', \"n't\", 'understand', '?', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def add_bert_tags(documents: List[List[str]], doc_tags: List[List[str]]):\n",
    "    updated_docs = [['[CLS]', *doc, '[SEP]'] for doc in documents]\n",
    "    updated_tags = [[0, *tags, \n",
    "] for tags in doc_tags]  # normally this would be <pad>, *tags, <pad>\n",
    "\n",
    "    return updated_docs, updated_tags\n",
    "\n",
    "bert_sentences, bert_targets = add_bert_tags(sentences, targets)\n",
    "\n",
    "print(bert_sentences[0])\n",
    "print(bert_targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ba4cdc6cdb2242b680d0df070c29339a",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "464903c512614bbf80472eb679ce9499",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 16,
    "execution_start": 1681926326666,
    "source_hash": "2faf5d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"[CLS] My dad just does n't understand ? [SEP]\",\n",
       " [101, 1422, 4153, 1198, 1674, 183, 112, 189, 2437, 136],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class BertDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, documents: List[List[str]], doc_tags: List[List[str]]):\n",
    "        self.docs = documents\n",
    "        self.doc_tags = doc_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sentence, tags = self.docs[idx], self.doc_tags[idx]\n",
    "\n",
    "        enc_sentence = []\n",
    "        enc_tags = []\n",
    "        is_heads = []\n",
    "        for word, tag in zip(sentence, tags):\n",
    "            tokens = bert_tokenizer.tokenize(word)\n",
    "            ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            tag = [tag] + [0] * (len(tokens) - 1)  # target label should only be assigned to head\n",
    "            is_head = [1] + [0] * (len(tokens) - 1)  # pay attention only to the first part of the word\n",
    "\n",
    "            enc_sentence += ids\n",
    "            enc_tags += tag\n",
    "            is_heads += is_head\n",
    "        \n",
    "        sentence = \" \".join(sentence)\n",
    "        return sentence, enc_sentence, enc_tags\n",
    "\n",
    "dataset_test = BertDataset([bert_sentences[0]], [bert_targets[0]])\n",
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "f40d655964194e8eaa1acfade34dd3a4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 4,
    "execution_start": 1681926326685,
    "source_hash": "1e724b8b"
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    batch_t = list(map(list, zip(*batch)))  # transpose\n",
    "    sentences, enc_sents, enc_targets = batch_t\n",
    "    max_len = max((map(len, enc_sents)))\n",
    "    \n",
    "    pad_sents = []\n",
    "    pad_targets = []\n",
    "    for enc_sent, enc_target in zip(enc_sents, enc_targets):\n",
    "        pad_sents.append(enc_sent + [0] * (max_len - len(enc_sent)))\n",
    "        pad_targets.append(enc_target + [0] * (max_len - len(enc_target)))\n",
    "    \n",
    "    pad_sents = torch.LongTensor(pad_sents)\n",
    "    pad_targets = torch.LongTensor(pad_targets)\n",
    "\n",
    "    return sentences, pad_sents, pad_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b4641fda37db4179a18a886d42a64cf0",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "a0b05fc1c2ad47fe876426282afe48c1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 305,
    "execution_start": 1681926326722,
    "source_hash": "15311f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N - number of samples, T - number of tokens, EMB - embedding dimension\n",
    "bert_model(torch.LongTensor([dataset_test[0][1]]))[0].shape  # (N, T, EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "e40ec59468d84b6bb66730298539eb49",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 5,
    "execution_start": 1681926327040,
    "source_hash": "3a61c6ad"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# TODO: come up with an original name for the model lol\n",
    "class SomeModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(768, out_features=2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (N, T)\n",
    "        \"\"\"\n",
    "        embeds = bert_model(inputs)[0]   # (N, T, 768)\n",
    "        preds = self.linear(embeds)  # (N, T, 2)\n",
    "        log_probs = F.softmax(preds, dim=2)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "14cdf21e16014fb09853a71009d115cd",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 6,
    "execution_start": 1681926327112,
    "source_hash": "24ce0219"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):          \n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        sent, enc_sent, enc_targets = batch\n",
    "\n",
    "        # predictions\n",
    "        pred_tags = model(inputs=enc_sent) # (N, T, 2)\n",
    "        pred_tags = pred_tags.view(-1, 2) # (N x T, 2)\n",
    "        \n",
    "        # true label for each word\n",
    "        targets = enc_targets.flatten()  # (N x T)\n",
    "\n",
    "        # loss\n",
    "        batch_loss = criterion(pred_tags, targets)\n",
    "        \n",
    "        # optimization\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "                        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i} loss: {batch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1706b67328544422a20403d63455ae92",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "854c66bcd8004e298d177e90b629b160",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 74,
    "execution_start": 1681926327122,
    "source_hash": "bb596e33"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "model = SomeModel()\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = BertDataset(\n",
    "    documents=bert_sentences,\n",
    "    doc_tags=bert_targets    \n",
    ")\n",
    "\n",
    "train_iter = data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_batch\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "4766f79816184a0e9e4e445914956733",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 8607,
    "execution_start": 1681926327178,
    "source_hash": "76577255"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [11], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m sent, enc_sent, enc_targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# predictions\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pred_tags \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_sent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (N, T, 2)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pred_tags \u001b[38;5;241m=\u001b[39m pred_tags\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_labels) \u001b[38;5;66;03m# (N x T, 2)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# true label for each word\u001b[39;00m\n",
      "File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [10], line 16\u001b[0m, in \u001b[0;36mSomeModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     14\u001b[0m embeds \u001b[38;5;241m=\u001b[39m bert_model(inputs)[\u001b[38;5;241m0\u001b[39m]   \u001b[38;5;66;03m# (N, T, 768)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(embeds)  \u001b[38;5;66;03m# (N, T, 2)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39msoftmax(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_probs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b2f14aee-af04-4db5-af55-57a3a58b9f40' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "46d7453ef7a841518deeb53b9fe33eb4",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
